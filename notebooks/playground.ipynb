{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Playground"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17167da3664942c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import necessary packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adb32e7e9ca3f3e4"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, LeakyReLU, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Lasso"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-15T01:23:08.086298300Z",
     "start_time": "2023-08-15T01:23:07.970487700Z"
    }
   },
   "id": "ef7ebd622869abf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Ingestion"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84806e9e76d17af7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"../data/raw/raw.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3450340e9c6de657"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8b5dfdffc414908"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the style of the visualization\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Prepare a list of columns to visualize\n",
    "cols_to_visualize = [\"Type\", \"Priority\", \"Story_Points\"]\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(nrows=len(cols_to_visualize), figsize=(12, 15))\n",
    "\n",
    "# Plot each column\n",
    "for i, col in enumerate(cols_to_visualize):\n",
    "    sns.countplot(y=data[col], ax=axs[i], order=data[col].value_counts().index)\n",
    "    axs[i].set_title(f'Distribution of {col}')\n",
    "    axs[i].set_xlabel('Count')\n",
    "    axs[i].set_ylabel(col)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "599a183ad942b712"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ab6c27b60b43484"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adjust the column to visualize\n",
    "cols_to_visualize[2] = \"Story_Point\"\n",
    "\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(nrows=len(cols_to_visualize), figsize=(12, 15))\n",
    "\n",
    "# Plot each column\n",
    "for i, col in enumerate(cols_to_visualize):\n",
    "    sns.countplot(y=data[col], ax=axs[i], order=data[col].value_counts().index)\n",
    "    axs[i].set_title(f'Distribution of {col}')\n",
    "    axs[i].set_xlabel('Count')\n",
    "    axs[i].set_ylabel(col)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8780b6538ce57c6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d455dadd1cd86da0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef0c5973359237b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_data = data.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# Calculate the percentage of missing data\n",
    "missing_percentage = (data.isnull().sum() / data.shape[0]).sort_values(ascending=False) * 100\n",
    "\n",
    "# Combine the missing data count and percentage into a DataFrame\n",
    "missing_df = pd.DataFrame({'Missing Count': missing_data, 'Missing Percentage (%)': missing_percentage})\n",
    "\n",
    "# Display the columns with missing data\n",
    "missing_df[missing_df['Missing Count'] > 0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cff8069c55769c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop columns with 100% missing values\n",
    "data.drop(['Timespent', 'Pull_Request_URL'], axis=1, inplace=True)\n",
    "\n",
    "# Introduce a binary column for Description_Code\n",
    "data['Has_Description_Code'] = data['Description_Code'].notnull().astype(int)\n",
    "\n",
    "# Drop the original Description_Code column\n",
    "data.drop('Description_Code', axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows of the modified dataset\n",
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b454aa1c65643ad1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "\n",
    "# Story_Point: Impute with median\n",
    "data['Story_Point'].fillna(data['Story_Point'].median(), inplace=True)\n",
    "\n",
    "# Priority: Impute with mode\n",
    "data['Priority'].fillna(data['Priority'].mode()[0], inplace=True)\n",
    "\n",
    "# Estimation_Date: Impute with median date\n",
    "data['Estimation_Date'].fillna(pd.to_datetime(data['Estimation_Date']).median(), inplace=True)\n",
    "\n",
    "# Check for remaining missing values\n",
    "remaining_missing = data.isnull().sum().sort_values(ascending=False)\n",
    "remaining_missing_df = pd.DataFrame({'Missing Count': remaining_missing})\n",
    "remaining_missing_df[remaining_missing_df['Missing Count'] > 0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57e99b7acf88ded7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Impute missing values for Description and Description_Text with \"No Description\"\n",
    "data['Description'].fillna(\"No Description\", inplace=True)\n",
    "data['Description_Text'].fillna(\"No Description\", inplace=True)\n",
    "\n",
    "# Check for remaining missing values\n",
    "remaining_missing = data.isnull().sum().sort_values(ascending=False)\n",
    "remaining_missing_df = pd.DataFrame({'Missing Count': remaining_missing})\n",
    "remaining_missing_df[remaining_missing_df['Missing Count'] > 0]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab56cf836a071214"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert the date columns to datetime format\n",
    "data['Creation_Date'] = pd.to_datetime(data['Creation_Date'])\n",
    "data['Estimation_Date'] = pd.to_datetime(data['Estimation_Date'])\n",
    "\n",
    "# Extract the time difference between Creation_Date and Estimation_Date\n",
    "data['Time_To_Estimate'] = (data['Estimation_Date'] - data['Creation_Date']).dt.days\n",
    "\n",
    "# Display the first few rows to check the new feature\n",
    "data[['Creation_Date', 'Estimation_Date', 'Time_To_Estimate']].head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f58c3b744a8f063"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One-hot encode the 'Type' and 'Priority' columns\n",
    "data_encoded = pd.get_dummies(data, columns=['Type', 'Priority'], drop_first=True)\n",
    "\n",
    "# Display the first few rows of the encoded dataset\n",
    "data_encoded.head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96e98980ae6e331b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature Selection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e12c69405d5d9eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "columns_to_drop = ['ID', 'Jira_ID', 'Issue_Key', 'URL', 'Title', 'Description', 'Description_Text', 'Creation_Date', 'Estimation_Date', 'Resolution_Date', 'Last_Updated', 'Repository_Name', 'Poject_Name']\n",
    "data_selected = data_encoded.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the first few rows of the dataset after dropping irrelevant columns\n",
    "data_selected.head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0d7190b9c8c08f2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate a correlation matrix\n",
    "correlation_matrix = data_selected.corr()\n",
    "\n",
    "# Get correlations with the target variable 'Story_Point'\n",
    "story_point_corr = correlation_matrix['Story_Point'].sort_values(ascending=False)\n",
    "\n",
    "# Display correlations\n",
    "story_point_corr\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10505a18b1f4c2a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate data for heatmap based on the provided correlation details\n",
    "story_point_corr_data = story_point_corr.to_frame()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(30, 10))\n",
    "sns.heatmap(story_point_corr_data, annot=True, cmap='coolwarm', cbar=True, square=True, linewidths=0.5)\n",
    "plt.title('Correlation with Story_Point')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40aa223c9f46c54c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the features (X) and the target (y)\n",
    "X = data_selected.drop(columns=['Story_Point', 'Status', 'Resolution'])\n",
    "y = data_selected['Story_Point']\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91513d27b40eea5e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "400107317b288cf6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the linear regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, rmse, mae, r2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c6a016beaeafe9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the data types of the training set\n",
    "non_numeric_columns = X_train.select_dtypes(include=['object']).columns\n",
    "non_numeric_columns\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb3546678994e3d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# One-hot encode the 'Repository_Name' and 'Poject_Name' columns\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=['Repository_Name', 'Poject_Name'], drop_first=True)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=['Repository_Name', 'Poject_Name'], drop_first=True)\n",
    "\n",
    "# Ensure consistent columns between train and test sets after encoding\n",
    "missing_cols = set(X_train_encoded.columns) - set(X_test_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    X_test_encoded[col] = 0\n",
    "\n",
    "X_test_encoded = X_test_encoded[X_train_encoded.columns]\n",
    "\n",
    "# Train the linear regression model again\n",
    "lr_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, rmse, mae, r2\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80e94648ec82beca"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Identify columns with NaN or infinite values in the training set\n",
    "nan_inf_columns = X_train_encoded.columns[X_train_encoded.isnull().any() | ~np.isfinite(X_train_encoded).all()]\n",
    "nan_inf_columns\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b9bcd3f65dc153a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Impute NaN values with a placeholder value (-1) for 'Assignee_ID' and 'Sprint_ID'\n",
    "X_train_encoded[['Assignee_ID', 'Sprint_ID']] = X_train_encoded[['Assignee_ID', 'Sprint_ID']].fillna(-1)\n",
    "X_test_encoded[['Assignee_ID', 'Sprint_ID']] = X_test_encoded[['Assignee_ID', 'Sprint_ID']].fillna(-1)\n",
    "\n",
    "# Train the linear regression model again\n",
    "lr_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lr_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, rmse, mae, r2\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb1d2922e465b870"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(y_test, y_pred)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red')\n",
    "plt.xlabel('Actual Story Points')\n",
    "plt.ylabel('Predicted Story Points')\n",
    "plt.title('Actual vs. Predicted Story Points')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87ac5fa42d0e9eee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Development"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c918ad01b21720e9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest Regressor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdecb948953a9c75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "mse_rf, rmse_rf, mae_rf, r2_rf\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ec0d2cb2995e35"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values for Random Forest Regressor\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(y_test, y_pred_rf)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red')\n",
    "plt.xlabel('Actual Story Points')\n",
    "plt.ylabel('Predicted Story Points')\n",
    "plt.title('Actual vs. Predicted Story Points (Random Forest)')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd50b4bdd8d2b12"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Boosting Regressor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14ee8542e1d78ee6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize and train the Gradient Boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_gb = gb_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the Gradient Boosting model\n",
    "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
    "rmse_gb = mean_squared_error(y_test, y_pred_gb, squared=False)\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "r2_gb = r2_score(y_test, y_pred_gb)\n",
    "\n",
    "mse_gb, rmse_gb, mae_gb, r2_gb\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afac87d36d9ae2d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values for Gradient Boosting Regressor\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(y_test, y_pred_gb)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red')\n",
    "plt.xlabel('Actual Story Points')\n",
    "plt.ylabel('Predicted Story Points')\n",
    "plt.title('Actual vs. Predicted Story Points (Gradient Boosting)')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "290a47dbfd2cf22a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Synthetic Data using GANs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b9a26fa9f57d437"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select a subset of features for simplicity\n",
    "selected_features = ['In_Progress_Minutes', 'Total_Effort_Minutes', 'Time_To_Estimate', 'Story_Point']\n",
    "gan_data = data_selected[selected_features]\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "gan_data_normalized = scaler.fit_transform(gan_data)\n",
    "\n",
    "# Convert the normalized data back to a dataframe\n",
    "gan_data_normalized_df = pd.DataFrame(gan_data_normalized, columns=selected_features)\n",
    "\n",
    "gan_data_normalized_df.head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee991e93c247a857"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GAN Parameters\n",
    "input_dim = len(selected_features)\n",
    "generator_output_dim = input_dim\n",
    "discriminator_output_dim = 1\n",
    "hidden_dim = 128\n",
    "\n",
    "# Build the generator\n",
    "generator = Sequential([\n",
    "    Dense(hidden_dim, input_dim=input_dim),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    BatchNormalization(momentum=0.8),\n",
    "    Dense(hidden_dim),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    BatchNormalization(momentum=0.8),\n",
    "    Dense(generator_output_dim, activation='tanh')\n",
    "])\n",
    "\n",
    "# Build the discriminator\n",
    "discriminator = Sequential([\n",
    "    Dense(hidden_dim, input_dim=generator_output_dim),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Dense(hidden_dim),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Dense(discriminator_output_dim, activation='sigmoid')\n",
    "])\n",
    "\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# The generator takes noise as input and generates samples\n",
    "z = tf.keras.layers.Input(shape=(input_dim,))\n",
    "sample = generator(z)\n",
    "\n",
    "# For the combined model, only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated samples as input and determines validity\n",
    "validity = discriminator(sample)\n",
    "\n",
    "# The combined model (stacked generator and discriminator) takes\n",
    "# noise as input => generates samples => determines validity \n",
    "combined = tf.keras.models.Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "combined.summary()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc0c127af847c230"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define Generator\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=noise_dim))\n",
    "    model.add(Dense(data_dim, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Define Discriminator\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=data_dim))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Compile models\n",
    "optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "generator = build_generator()\n",
    "\n",
    "# Combined model (for training the generator)\n",
    "z = tf.keras.layers.Input(shape=(noise_dim,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "validity = discriminator(img)\n",
    "combined = tf.keras.models.Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # Train Discriminator\n",
    "    idx = np.random.randint(0, real_data.shape[0], batch_size)\n",
    "    real_samples = real_data[idx]\n",
    "    noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "    generated_samples = generator.predict(noise)\n",
    "    real_labels = np.ones((batch_size, 1))\n",
    "    fake_labels = np.zeros((batch_size, 1))\n",
    "    d_loss_real = discriminator.train_on_batch(real_samples, real_labels)\n",
    "    d_loss_fake = discriminator.train_on_batch(generated_samples, fake_labels)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # Train Generator\n",
    "    noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "    valid_labels = np.ones((batch_size, 1))\n",
    "    g_loss = combined.train_on_batch(noise, valid_labels)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8216082414429881"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support Vector Regression (SVR)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "249b1c41dff147a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SVR requires feature scaling for better performance\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_encoded)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
    "X_test_scaled = scaler_X.transform(X_test_encoded)\n",
    "\n",
    "# Train the SVR model\n",
    "svr_model = SVR(kernel='rbf')\n",
    "svr_model.fit(X_train_scaled, y_train_scaled.ravel())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svr = svr_model.predict(X_test_scaled)\n",
    "y_pred_svr = scaler_y.inverse_transform(y_pred_svr)  # Inverse transform to original scale\n",
    "\n",
    "# Evaluate the SVR model\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "rmse_svr = mean_squared_error(y_test, y_pred_svr, squared=False)\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "mse_svr, rmse_svr, mae_svr, r2_svr\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec1b911ed9ac413"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reshape the predictions and perform inverse transformation\n",
    "y_pred_svr_reshaped = y_pred_svr.reshape(-1, 1)\n",
    "y_pred_svr_original = scaler_y.inverse_transform(y_pred_svr_reshaped)\n",
    "\n",
    "# Evaluate the SVR model again\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr_original)\n",
    "rmse_svr = mean_squared_error(y_test, y_pred_svr_original, squared=False)\n",
    "mae_svr = mean_absolute_error(y_test, y_pred_svr_original)\n",
    "r2_svr = r2_score(y_test, y_pred_svr_original)\n",
    "\n",
    "mse_svr, rmse_svr, mae_svr, r2_svr"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbe373ccddb1f9de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparison with Previous SVR Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2421f0894ac9e928"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values for SVR\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(y_test, y_pred_svr_original.ravel())\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red')\n",
    "plt.xlabel('Actual Story Points')\n",
    "plt.ylabel('Predicted Story Points')\n",
    "plt.title('Actual vs. Predicted Story Points (Support Vector Regression)')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c3a3d7d1fe7db4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ridge Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f901aa9d5c6f9b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize and train the Ridge Regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_ridge = ridge_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the Ridge model\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "rmse_ridge = mean_squared_error(y_test, y_pred_ridge, squared=False)\n",
    "mae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n",
    "r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "\n",
    "mse_ridge, rmse_ridge, mae_ridge, r2_ridge\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d504894dee1bb15e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Lasso Regression"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4a2b165cb4e3acf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize and train the Lasso Regression model\n",
    "lasso_model = Lasso(alpha=0.1)\n",
    "lasso_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_lasso = lasso_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the Lasso model\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "rmse_lasso = mean_squared_error(y_test, y_pred_lasso, squared=False)\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "mse_lasso, rmse_lasso, mae_lasso, r2_lasso\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2d645d3d4998248"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparison with Previous Lasso Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb879b1326b9e094"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values for Lasso Regression\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(y_test, y_pred_lasso)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red')\n",
    "plt.xlabel('Actual Story Points')\n",
    "plt.ylabel('Predicted Story Points')\n",
    "plt.title('Actual vs. Predicted Story Points (Lasso Regression)')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21e9db91e0c2d54b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Neural Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "125863af268851e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize and train a simple neural network\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(128, 64), max_iter=1000, random_state=42)\n",
    "nn_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nn = nn_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the neural network model\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "rmse_nn = mean_squared_error(y_test, y_pred_nn, squared=False)\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "r2_nn = r2_score(y_test, y_pred_nn)\n",
    "\n",
    "mse_nn, rmse_nn, mae_nn, r2_nn\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f523de03b3ec7191"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values for Neural Network\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(y_test, y_pred_nn)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--', color='red')\n",
    "plt.xlabel('Actual Story Points')\n",
    "plt.ylabel('Predicted Story Points')\n",
    "plt.title('Actual vs. Predicted Story Points (Neural Network)')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7e3106f741f2c79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fuzzy Grey Relational Analysis (GRA)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3aabb1229dc03da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grey_relational_coefficient(data, ref_seq, rho=0.5):\n",
    "    \"\"\"\n",
    "    Compute the Grey Relational Coefficient for data against a reference sequence.\n",
    "    \"\"\"\n",
    "    min_diff = np.min(np.abs(data - ref_seq))\n",
    "    max_diff = np.max(np.abs(data - ref_seq))\n",
    "    GRC = (min_diff + rho * max_diff) / (np.abs(data - ref_seq) + rho * max_diff)\n",
    "    return GRC\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = (data_selected - data_selected.min()) / (data_selected.max() - data_selected.min())\n",
    "ref_seq = normalized_data['Story_Point'].values\n",
    "\n",
    "# Calculate Grey Relational Coefficients for each feature\n",
    "grc_values = {}\n",
    "for column in normalized_data.columns:\n",
    "    if column != 'Story_Point':\n",
    "        grc_values[column] = np.mean(grey_relational_coefficient(normalized_data[column].values, ref_seq))\n",
    "\n",
    "# Sort features by Grey Relational Grade\n",
    "sorted_grc_values = dict(sorted(grc_values.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_grc_values\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d4af2d54e9fa276"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter out non-numeric columns for normalization\n",
    "numeric_data = data_selected.select_dtypes(include=[np.number])\n",
    "\n",
    "# Normalize the numeric data\n",
    "normalized_data = (numeric_data - numeric_data.min()) / (numeric_data.max() - numeric_data.min())\n",
    "ref_seq = normalized_data['Story_Point'].values\n",
    "\n",
    "# Calculate Grey Relational Coefficients for each feature\n",
    "grc_values = {}\n",
    "for column in normalized_data.columns:\n",
    "    if column != 'Story_Point':\n",
    "        grc_values[column] = np.mean(grey_relational_coefficient(normalized_data[column].values, ref_seq))\n",
    "\n",
    "# Sort features by Grey Relational Grade\n",
    "sorted_grc_values = dict(sorted(grc_values.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_grc_values\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "411aaba577aa5e9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Select the top 10 features based on GRG values\n",
    "top_features = list(sorted_grc_values.keys())[:10]\n",
    "\n",
    "# Extract these features from the original dataset\n",
    "selected_data_top = data_selected[top_features + ['Story_Point']]\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_top = selected_data_top.drop(columns=['Story_Point'])\n",
    "y_top = selected_data_top['Story_Point']\n",
    "\n",
    "# Use the same split ratio and random state for consistency\n",
    "X_train_top, X_test_top, y_train_top, y_test_top = train_test_split(X_top, y_top, test_size=0.3, random_state=42)\n",
    "\n",
    "# Encoding the categorical features\n",
    "X_train_encoded_top = pd.get_dummies(X_train_top, drop_first=True)\n",
    "X_test_encoded_top = pd.get_dummies(X_test_top, drop_first=True)\n",
    "\n",
    "# Aligning the train and test data for consistent columns\n",
    "X_train_encoded_top, X_test_encoded_top = X_train_encoded_top.align(X_test_encoded_top, join='left', axis=1)\n",
    "\n",
    "# Fill any new NaN values (due to alignment) with 0\n",
    "X_train_encoded_top.fillna(0, inplace=True)\n",
    "X_test_encoded_top.fillna(0, inplace=True)\n",
    "\n",
    "X_train_encoded_top.shape, X_test_encoded_top.shape\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bbf622f328b16799"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Regressor with Fuzzy Grey Relational Analysis (GRA)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf0a7382d87ac2c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the Random Forest Regressor using top-ranked features\n",
    "rf_model_top = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model_top.fit(X_train_encoded_top, y_train_top)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_rf_top = rf_model_top.predict(X_test_encoded_top)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "mse_rf_top = mean_squared_error(y_test_top, y_pred_rf_top)\n",
    "rmse_rf_top = mean_squared_error(y_test_top, y_pred_rf_top, squared=False)\n",
    "mae_rf_top = mean_absolute_error(y_test_top, y_pred_rf_top)\n",
    "r2_rf_top = r2_score(y_test_top, y_pred_rf_top)\n",
    "\n",
    "mse_rf_top, rmse_rf_top, mae_rf_top, r2_rf_top\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f473d5b7fa4ff79c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient Boosting Regression with Fuzzy Grey Relational Analysis (GRA)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e51c3ddd743fdb6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the Gradient Boosting Regressor using top-ranked features\n",
    "gb_model_top = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model_top.fit(X_train_encoded_top, y_train_top)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_gb_top = gb_model_top.predict(X_test_encoded_top)\n",
    "\n",
    "# Evaluate the Gradient Boosting model\n",
    "mse_gb_top = mean_squared_error(y_test_top, y_pred_gb_top)\n",
    "rmse_gb_top = mean_squared_error(y_test_top, y_pred_gb_top, squared=False)\n",
    "mae_gb_top = mean_absolute_error(y_test_top, y_pred_gb_top)\n",
    "r2_gb_top = r2_score(y_test_top, y_pred_gb_top)\n",
    "\n",
    "mse_gb_top, rmse_gb_top, mae_gb_top, r2_gb_top\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abf10ac85f599bcd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Support Vector Machine Regressor with Fuzzy Grey Relational Analysis (GRA)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "805bf9ded02d2f24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale the data for SVR using top-ranked features\n",
    "X_train_scaled_top = scaler_X.fit_transform(X_train_encoded_top)\n",
    "y_train_scaled_top = scaler_y.fit_transform(y_train_top.values.reshape(-1, 1))\n",
    "X_test_scaled_top = scaler_X.transform(X_test_encoded_top)\n",
    "\n",
    "# Train the SVR model\n",
    "svr_model_top = SVR(kernel='rbf')\n",
    "svr_model_top.fit(X_train_scaled_top, y_train_scaled_top.ravel())\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_svr_top = svr_model_top.predict(X_test_scaled_top)\n",
    "y_pred_svr_top_original = scaler_y.inverse_transform(y_pred_svr_top.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the SVR model\n",
    "mse_svr_top = mean_squared_error(y_test_top, y_pred_svr_top_original)\n",
    "rmse_svr_top = mean_squared_error(y_test_top, y_pred_svr_top_original, squared=False)\n",
    "mae_svr_top = mean_absolute_error(y_test_top, y_pred_svr_top_original)\n",
    "r2_svr_top = r2_score(y_test_top, y_pred_svr_top_original)\n",
    "\n",
    "mse_svr_top, rmse_svr_top, mae_svr_top, r2_svr_top\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0b4f19d544f84d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41e68c1bb667fa07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=RandomForestRegressor(random_state=42),\n",
    "                           param_grid=param_grid,\n",
    "                           cv=3,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2,\n",
    "                           scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit to the training data\n",
    "grid_search.fit(X_train_encoded_top, y_train_top)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "best_params\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b9152b1e8b3fbc1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Random Forest Regressor with Hyperparameter Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0b55203124d9766"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the Random Forest model with the best hyperparameters\n",
    "best_rf_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_rf_model.fit(X_train_encoded_top, y_train_top)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_best_rf = best_rf_model.predict(X_test_encoded_top)\n",
    "\n",
    "# Evaluate the optimized Random Forest model\n",
    "mse_best_rf = mean_squared_error(y_test_top, y_pred_best_rf)\n",
    "rmse_best_rf = mean_squared_error(y_test_top, y_pred_best_rf, squared=False)\n",
    "mae_best_rf = mean_absolute_error(y_test_top, y_pred_best_rf)\n",
    "r2_best_rf = r2_score(y_test_top, y_pred_best_rf)\n",
    "\n",
    "mse_best_rf, rmse_best_rf, mae_best_rf, r2_best_rf\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d75cf18e5086ef5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e557bbf3bfc8471d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values for the optimized Random Forest model\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(y_test_top, y_pred_best_rf)\n",
    "plt.plot([min(y_test_top), max(y_test_top)], [min(y_test_top), max(y_test_top)], '--', color='red')\n",
    "plt.xlabel('Actual Story Points')\n",
    "plt.ylabel('Predicted Story Points')\n",
    "plt.title('Actual vs. Predicted Story Points (Optimized Random Forest)')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9714d0b78156a06f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
